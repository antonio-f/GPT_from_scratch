{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is based on A. Karpathy's [code](https://github.com/karpathy/nanoGPT).\n",
    "\n",
    "The notebook contains the code for a simple bigram language model trained on GPT at character-level. The transformer trains on some context (sequence of characters) taken from Shakespeare's works and then it tries to predict/generate sequences that are likely to come later in the sequence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries and set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25e65d1cb70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64 # embedding dimension\n",
    "n_head = 4 # number of heads\n",
    "n_layer = 4 # number of layers\n",
    "dropout = 0.0\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get input text and read\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's look at the first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create string encoding/decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, `stoi` (string to index dictionary) is the following: \n",
    "\n",
    "```{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the encoding/decoding functions have been defined, let's try to encode some text and then decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42]\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"hello world\"))\n",
    "print(decode(encode(\"hello world\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "\n",
    "# put all data into a PyTorch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "# creating train and test datasets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing a block of train data\n",
    "# (block size is 32 by default)\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47]) the target: 64\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43]) the target: 52\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52]) the target: 10\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10]) the target: 0\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0]) the target: 14\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43]) the target: 44\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44]) the target: 53\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53]) the target: 56\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1]) the target: 61\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1]) the target: 54\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54]) the target: 56\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56]) the target: 53\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53]) the target: 41\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43]) the target: 43\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43]) the target: 42\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42]) the target: 1\n"
     ]
    }
   ],
   "source": [
    "# exploring test and train data\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for creating batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell contains a function (it will be used later) for generating a batch of data (inputs+targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# testing get_batch function\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[58, 53,  1, 41, 53, 56, 56, 59, 54, 58,  1, 39,  1, 51, 39, 52,  5, 57,\n",
      "          1, 61, 47, 44, 43,  1, 47, 57,  0, 61, 46, 43, 52,  1],\n",
      "        [49,  1, 39, 52,  1, 53, 39, 58, 46,  1, 40, 63,  1, 20, 47, 51,  6,  0,\n",
      "         32, 46, 43,  1, 59, 52, 47, 58, 63,  1, 58, 46, 43,  1],\n",
      "        [59, 50, 42,  1, 58, 46, 53, 59,  1, 61, 43, 56, 58,  1, 57, 53,  1, 58,\n",
      "         53, 53,  2,  0,  0, 24, 33, 15, 21, 27, 10,  0, 35, 43],\n",
      "        [ 8,  0,  0, 35, 13, 30, 35, 21, 15, 23, 10,  0, 28, 56, 53, 60, 43,  1,\n",
      "         47, 58,  6,  1, 20, 43, 52, 56, 63,  6,  1, 39, 52, 42],\n",
      "        [58,  1, 57, 46, 43,  8,  0,  0, 32, 30, 13, 26, 21, 27, 10,  0, 18, 53,\n",
      "         56,  1, 61, 46, 39, 58,  1, 56, 43, 39, 57, 53, 52,  6],\n",
      "        [56, 61, 47, 41, 49,  6,  1, 50, 43, 58,  1, 47, 58,  1, 40, 43, 11,  0,\n",
      "         18, 53, 56,  1, 47, 52,  1, 58, 46, 63,  1, 57, 46, 53],\n",
      "        [25, 10,  0, 35, 47, 58, 46, 42, 56, 39, 61,  1, 63, 53, 59,  1, 46, 43,\n",
      "         52, 41, 43,  6,  1, 51, 63,  1, 50, 53, 56, 42,  6,  1],\n",
      "        [43, 57, 58,  1, 61, 47, 58, 46,  1, 58, 46, 63,  1, 44, 56, 53, 64, 43,\n",
      "         52,  1, 39, 42, 51, 53, 52, 47, 58, 47, 53, 52,  0, 25],\n",
      "        [47, 52, 41, 43,  1, 58, 46, 53, 59,  6,  1, 41, 56, 43, 39, 58, 43, 42,\n",
      "          1, 58, 53,  1, 40, 43,  1, 39, 61, 43, 42,  1, 40, 63],\n",
      "        [53, 52, 57,  8,  0,  0, 34, 27, 24, 33, 25, 26, 21, 13, 10,  0, 27,  6,\n",
      "          1, 52, 53,  1, 51, 53, 56, 43,  6,  1, 52, 53,  1, 51],\n",
      "        [53, 59, 56,  1, 47, 51, 54, 43, 42, 47, 51, 43, 52, 58,  8,  1, 18, 53,\n",
      "         56,  1, 58, 46, 43,  1, 42, 43, 39, 56, 58, 46,  6,  0],\n",
      "        [53, 58, 46, 47, 52, 45,  8,  0,  0, 16, 33, 23, 17,  1, 27, 18,  1, 37,\n",
      "         27, 30, 23, 10,  0, 26, 53,  1, 51, 39, 58, 58, 43, 56],\n",
      "        [53, 61,  1, 41, 43, 56, 43, 51, 53, 52, 47, 53, 59, 57,  6,  1, 57, 53,\n",
      "         50, 43, 51, 52,  1, 39, 52, 42,  1, 59, 52, 43, 39, 56],\n",
      "        [50, 50,  1, 57, 58, 47, 50, 50,  1, 50, 47, 60, 43,  1, 41, 46, 39, 57,\n",
      "         58, 43, 12,  0,  0, 30, 27, 25, 17, 27, 10,  0, 31, 46],\n",
      "        [21, 21, 10,  0, 13, 63,  6,  1, 47, 44,  1, 63, 53, 59, 56, 57, 43, 50,\n",
      "         44,  5, 57,  1, 56, 43, 51, 43, 51, 40, 56, 39, 52, 41],\n",
      "        [59, 58,  1, 47, 52,  1, 46, 43, 56,  1, 58, 43, 52, 42, 43, 56,  1, 46,\n",
      "         43, 39, 56, 58,  1, 58, 46, 43,  1, 39, 57, 54, 47, 56]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the loss function definition. It referes to the model loss that comes out from  `model(X,Y)`, defined using cross entropy loss between predictions and true targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function definition\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building GPT architecture - Attention module blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a [link](https://becominghuman.ai/multi-head-attention-f2cfb4060e9c) to a page on the inner functioning of Multihead Attention. The page is not about the masked version of Multihead Attention (a variation illustrated [here](https://www.coursera.org/lecture/attention-models-in-nlp/masked-self-attention-AMz8y))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the original notes on attention from A. Karpathy:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.tril` defines the lower triangular matrix used to create masking. The code for the Multihead Attention module is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple linear module (fully connected layer)\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building GPT architecture - Transformer decoder block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the implementation of the GPT Transformer block, as illustrated in the original paper. It is a **decoder** module, although it looks more like the Transformer's **encoder** module. The original paper architecture uses 12 such blocks, our implementation uses just 4 blocks by default (obviously, it can be modified in hyperparams cell).  \n",
    "<center>\n",
    "\n",
    "![Alt text](block.png)\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple bigram language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models that assign probabilities to sequences of words are called **language models**. Beginning with the task of computing $\\mathrm{P}(w \\mid h)$, the probability of a *word* $w$ given some *history* $h$, one can consider history $h$ = \"the heater is so hot that\" and the probability that the next word is \"the\"\n",
    "\n",
    "$$\n",
    "\\mathrm{P}(\\text{the} \\mid \\text{the heater is so hot that})\n",
    "$$\n",
    "\n",
    "and compute it as relative frequency counts: take a very large corpus, count the number of times we see  \"the heater is so hot that\",\n",
    "and count the number of times this is followed by \"the\". However this is not feasible. Usually *naive methods* are used and this implies the use of some simplifications.\n",
    "\n",
    "**Bigram model** is easy to implement but very naive. For example, it approximates the probability of a word $w_n$ given all the previous words \n",
    "\n",
    "$$ \n",
    "\\mathrm{P}(w_n \\mid w_1, w_2, \\dots, w_{n−1}) \n",
    "$$\n",
    "\n",
    "by using only the conditional probability of the preceding word $P(w_n \\mid w_{n−1})$. In other words, instead of computing the probability\n",
    "\n",
    "$$\n",
    "\\mathrm{P}(\\text{the} \\mid \\text{the main heater is so hot that})\\,,\n",
    "$$\n",
    "\n",
    "we approximate it with the probability\n",
    "\n",
    "$$\n",
    "\\mathrm{P}(\\text{the} \\mid \\text{that})\\,.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains code for a basic bigram language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "# define a bigram model instance\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4049, val loss 4.3996\n",
      "step 100: train loss 2.6715, val loss 2.6830\n",
      "step 200: train loss 2.5027, val loss 2.4981\n",
      "step 300: train loss 2.4162, val loss 2.4296\n",
      "step 400: train loss 2.3455, val loss 2.3579\n",
      "step 500: train loss 2.3018, val loss 2.3208\n",
      "step 600: train loss 2.2375, val loss 2.2534\n",
      "step 700: train loss 2.2067, val loss 2.2189\n",
      "step 800: train loss 2.1571, val loss 2.1790\n",
      "step 900: train loss 2.1160, val loss 2.1491\n",
      "step 1000: train loss 2.0883, val loss 2.1196\n",
      "step 1100: train loss 2.0565, val loss 2.1083\n",
      "step 1200: train loss 2.0228, val loss 2.0721\n",
      "step 1300: train loss 2.0102, val loss 2.0499\n",
      "step 1400: train loss 1.9758, val loss 2.0222\n",
      "step 1500: train loss 1.9522, val loss 2.0192\n",
      "step 1600: train loss 1.9308, val loss 2.0215\n",
      "step 1700: train loss 1.9261, val loss 2.0052\n",
      "step 1800: train loss 1.8849, val loss 1.9826\n",
      "step 1900: train loss 1.8781, val loss 1.9652\n",
      "step 2000: train loss 1.8612, val loss 1.9721\n",
      "step 2100: train loss 1.8480, val loss 1.9516\n",
      "step 2200: train loss 1.8297, val loss 1.9348\n",
      "step 2300: train loss 1.8255, val loss 1.9350\n",
      "step 2400: train loss 1.8108, val loss 1.9106\n",
      "step 2500: train loss 1.7922, val loss 1.9278\n",
      "step 2600: train loss 1.7959, val loss 1.9242\n",
      "step 2700: train loss 1.7892, val loss 1.9175\n",
      "step 2800: train loss 1.7797, val loss 1.9003\n",
      "step 2900: train loss 1.7733, val loss 1.9023\n",
      "step 3000: train loss 1.7638, val loss 1.8924\n",
      "step 3100: train loss 1.7414, val loss 1.8941\n",
      "step 3200: train loss 1.7228, val loss 1.8783\n",
      "step 3300: train loss 1.7310, val loss 1.8842\n",
      "step 3400: train loss 1.7273, val loss 1.8693\n",
      "step 3500: train loss 1.7094, val loss 1.8682\n",
      "step 3600: train loss 1.7021, val loss 1.8717\n",
      "step 3700: train loss 1.7022, val loss 1.8591\n",
      "step 3800: train loss 1.6998, val loss 1.8721\n",
      "step 3900: train loss 1.6910, val loss 1.8462\n",
      "step 4000: train loss 1.6850, val loss 1.8359\n",
      "step 4100: train loss 1.6859, val loss 1.8426\n",
      "step 4200: train loss 1.6798, val loss 1.8380\n",
      "step 4300: train loss 1.6793, val loss 1.8259\n",
      "step 4400: train loss 1.6823, val loss 1.8403\n",
      "step 4500: train loss 1.6674, val loss 1.8283\n",
      "step 4600: train loss 1.6632, val loss 1.8111\n",
      "step 4700: train loss 1.6569, val loss 1.8141\n",
      "step 4800: train loss 1.6427, val loss 1.8191\n",
      "step 4900: train loss 1.6428, val loss 1.8185\n",
      "step 4999: train loss 1.6450, val loss 1.8117\n"
     ]
    }
   ],
   "source": [
    "max_iters=5000 # training iterations\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see what our model is capable of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "And them toward I comes and to duke I uDpeacely a tivoness;\n",
      "Gravilgland, I'll must of that the good of utworder him,\n",
      "Nord man to in Istalent, sure her of the found Hurder the would,\n",
      "That is compoon, go:\n",
      "Citivioitiol speed spiritity? friend noths?\n",
      "Go, a his his love.\n",
      "\n",
      "POLIXENES:\n",
      "What with the will by us\n",
      "by thou sover conursedful bittaling.\n",
      "\n",
      "KING RICHARD II:\n",
      "good'd will be in that let miting.\n",
      "\n",
      "First Leward a have.\n",
      "No, thou let\n",
      "be expulent spect thou, fryself who wands\n",
      "That not is forgiry's of your slord.\n",
      "\n",
      "Londied Y:\n",
      "That sweeth set hath. Wry, passes us.\n",
      "\n",
      "LUCENTIO:\n",
      "He wouldse in some me and proady's!\n",
      "Your slory the hath a fool will me the conce I later,\n",
      "Stay thy senvereal and lill still, paint off with not like to propuake that\n",
      "TErwapting it Andlercuping, thence:\n",
      "No more it? would I has thou,\n",
      "Aur folly such the was I'll bear conching-sconcears at and his long worning;\n",
      "Condlarly, Milier.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "\n",
      "GRUCEN:\n",
      "Why, daubjer, stravoion, now or chem desmen with\n",
      "That the boldy; lose to enth to the pilen good it that would the farther,\n",
      "And than wark you, bany, I us orlcland with thee,\n",
      "And countenty to bear healt to proted Plain,\n",
      "Whithou would I wish the measure? what you, but supper-longet and\n",
      "of the light my lord, he schearst reant.\n",
      "\n",
      "LUCIO:\n",
      "And I little to my love\n",
      "Dothonius to hate the lie, then thy lame,\n",
      "Ham he offerds frult his name him:\n",
      "The stear, it you: and do him\n",
      "it no.\n",
      "3shall! I play whold not suck upon pury And, I would; I brived;\n",
      "Relet be a proted to friend\n",
      "And tooken, prown you: I have sight\n",
      "the Rabution sknit, must\n",
      "Of the fastest of yot rese, and peopnent.\n",
      "\n",
      "Second.\n",
      "\n",
      "PARENCE:\n",
      "Where Capperse!\n",
      "\n",
      "JOHN RICHARD:\n",
      "Dosay me.\n",
      "\n",
      "Sird:\n",
      "Efteren, Is is an themmptare wo\n",
      "\n",
      "First both,\n",
      "We'll we ussonget with requment, us I with\n",
      "Hord fire unfeed them execread by?\n",
      "\n",
      "ROMEO:\n",
      "His my littenty this fall more is in me.\n",
      "\n",
      "POMERS:\n",
      "You staves that hi unpray plateful breath,\n",
      "Of my dread with her, maintry all, not to repook to me.\n",
      "\n",
      "Servo'd Secreace in Citizen:\n",
      "Ontaguateo, that I with two blefo\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
